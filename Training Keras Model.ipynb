{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for consolidating all the CSVs generated from the \"Environment - Creating Training Data\" notebook. We consolidate the raw files into an SQL database because they are too large to fit into memory. We can then complete transformations on the raw data and restore that into the SQL database as a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import advanced_activations\n",
    "from keras import initializers\n",
    "\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data\\TrainOn\\20160616_2  - close enough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\envs\\sb\\lib\\site-packages\\pandas\\core\\generic.py:1362: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  chunksize=chunksize, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data\\TrainOn\\20160617\n",
      "training_data\\TrainOn\\20160619\n",
      "training_data\\TrainOn\\20160620\n",
      "training_data\\TrainOn\\20160621\n",
      "training_data\\TrainOn\\20170622\n",
      "training_data\\TrainOn\\20170623\n",
      "training_data\\TrainOn\\20170626 - model\n",
      "training_data\\TrainOn\\20170627 - model\n"
     ]
    }
   ],
   "source": [
    "# Load the generated CSV files to an SQL DB\n",
    "\n",
    "con = sqlite3.connect(\"SquareBrawlTrainingRawSQLite.db\")\n",
    "\n",
    "folders = glob.glob('training_data\\\\TrainOn\\*')\n",
    "\n",
    "#trains = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    trains = pd.DataFrame()\n",
    "    folder = folder + '\\\\*'\n",
    "    files = glob.glob(folder)\n",
    "    sfiles = sorted(files, key=os.path.getctime)\n",
    "    for sfile in sfiles:\n",
    "        train = pd.read_csv(sfile)\n",
    "        trains = pd.concat([trains, train])\n",
    "    \n",
    "    trains = trains.reset_index().drop('index',axis=1)\n",
    "    trains.to_sql(name='SquareBrawlTrainingRaw', con=con, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def score_delta_index(chunk, column_name):\n",
    "    '''\n",
    "    Grabs the row index for when in the \n",
    "    incrememnts in the players score happen.\n",
    "    Allows us to circumvent the reward \n",
    "    allocation problem\n",
    "    '''\n",
    "    rows = chunk.shape[0]\n",
    "    prev_score = 0\n",
    "    prev_row = 0\n",
    "    kills_index = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        if chunk.loc[row, column_name] != prev_score:\n",
    "            kills_index.append([prev_row, row])\n",
    "            prev_score = chunk.loc[row, column_name]\n",
    "            prev_row = row + 1\n",
    "    del kills_index[0] # delete [0,0]\n",
    "    \n",
    "    return kills_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_lambda_adjustments(chunk, adjustment_column, kills_index, reward, decay, td_lookback):\n",
    "    '''\n",
    "    Allows for the artificial adjustment of the reward \n",
    "    metric by allocating discounted value back based on\n",
    "    player kills\n",
    "    '''\n",
    "    # TD lamnda adjustments for kills\n",
    "    for kill in kills_index:\n",
    "        killpoint = kill[1]\n",
    "        exponent = td_lookback - 1\n",
    "        if killpoint - td_lookback < 0:\n",
    "            continue\n",
    "        else:\n",
    "            for row in range(killpoint - td_lookback, killpoint):\n",
    "                chunk.loc[row, adjustment_column] = reward * decay ** exponent\n",
    "                exponent -= 1\n",
    "        \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to SQL DB and pull from raw table\n",
    "\n",
    "con = sqlite3.connect(\"SquareBrawlTrainingRawSQLite.db\")\n",
    "\n",
    "SQL_STATEMENT = '''SELECT\n",
    "    episode,\n",
    "    p1_x,\n",
    "    \" p1_y\",\n",
    "    p2_x,\n",
    "    \" p2_y\",\n",
    "    right_of, \n",
    "    left_of, \n",
    "    above, \n",
    "    below, \n",
    "    x_clear, \n",
    "    y_clear, \n",
    "    cd1,\n",
    "    cd2, \n",
    "    aim_down, \n",
    "    aim_left, \n",
    "    aim_up, \n",
    "    aim_right,\n",
    "    action,\n",
    "    p1_score,\n",
    "    p2_score\n",
    "FROM\n",
    "SquareBrawlTrainingRaw\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTABLES\\n\\nTD25_decay9_pos_neg_reward_1_looking_reward\\nTD25_decay9_pos_neg_reward_1\\nTD5_decay9_pos_neg_reward_1_looking_reward\\nTD10_decay9_pos_neg_reward_1_looking_reward\\nshort_TD4_decay06_pos_neg_reward_1_looking_reward\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TABLES\n",
    "\n",
    "TD25_decay9_pos_neg_reward_1_looking_reward\n",
    "TD25_decay9_pos_neg_reward_1\n",
    "TD5_decay9_pos_neg_reward_1_looking_reward\n",
    "TD10_decay9_pos_neg_reward_1_looking_reward\n",
    "short_TD4_decay06_pos_neg_reward_1_looking_reward\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aim_reward(df, reward_value):\n",
    "    '''\n",
    "    Creates reward if the agent is pointing their \n",
    "    gun in the direction of the opponent \n",
    "    '''\n",
    "    above = df['above'] * df['aim_up']\n",
    "    below = df['below'] * df['aim_down']\n",
    "    right = df['right_of'] * df['aim_right']\n",
    "    left = df['left_of'] * df['aim_left']\n",
    "    \n",
    "    df['aim_rewards'] = (above + below)*df['y_clear'] + (right + left)*df['x_clear']\n",
    "    df['aim_rewards'] = df['aim_rewards'].apply(lambda x: reward_value if x == 1 else 0)\n",
    "    df['aim_rewards'] = df['aim_rewards'].shift(-1).fillna(0)\n",
    "    \n",
    "    return df['aim_rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\envs\\sb\\lib\\site-packages\\pandas\\core\\generic.py:1362: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  chunksize=chunksize, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 0\n",
      "Saving 1\n",
      "Saving 2\n",
      "Saving 3\n",
      "Saving 4\n",
      "Saving 5\n",
      "Saving 6\n"
     ]
    }
   ],
   "source": [
    "#-------TD LAMBDA REWARDS ADJUSTED---------#\n",
    "td_lookback = 4\n",
    "decay = 0.6\n",
    "reward_kill = 1\n",
    "reward_death = -1\n",
    "\n",
    "\n",
    "# load database with all raw training_data\n",
    "chunksizes = 500000\n",
    "training_chunks_raw = pd.read_sql(SQL_STATEMENT, con, chunksize=chunksizes)\n",
    "\n",
    "#create new database for adjusted rewards\n",
    "td_lambda_storage = sqlite3.connect(\"SquareBrawlSQLite_AdjustedRewards.db\")\n",
    "\n",
    "total_kills = []\n",
    "total_deaths = []\n",
    "episode = 0\n",
    "\n",
    "for chunk in training_chunks_raw:\n",
    "    chunk['reward_kill_adjusted'] = 0\n",
    "    chunk['reward_death_adjusted'] = 0\n",
    "    chunk['aim_rewards'] = 0\n",
    "    \n",
    "    #chunk = chunk[chunk['p2_score']<50]\n",
    "    chunk = chunk.reset_index().drop('index',axis=1)\n",
    "    \n",
    "    # rewards for looking at opponent\n",
    "    chunk['aim_rewards'] = aim_reward(chunk, 0.01)\n",
    "    \n",
    "    # grab kills\n",
    "    kills_index = score_delta_index(chunk,'p1_score')\n",
    "    total_kills.extend(kills_index)\n",
    "    chunk = td_lambda_adjustments(chunk, 'reward_kill_adjusted', kills_index, reward_kill, decay, td_lookback)\n",
    "    \n",
    "    # grab deaths\n",
    "    deaths_index = score_delta_index(chunk, 'p2_score')\n",
    "    total_deaths.extend(deaths_index)\n",
    "    chunk = td_lambda_adjustments(chunk, 'reward_death_adjusted', deaths_index, reward_death, decay, td_lookback)\n",
    "    \n",
    "    chunk['adjusted_rewards_total'] = chunk['reward_kill_adjusted'] + chunk['reward_death_adjusted']\n",
    "\n",
    "    chunk.to_sql(name='short_TD4_decay06_pos_neg_reward_1_looking_reward', con=td_lambda_storage, if_exists='append')\n",
    "    \n",
    "    print('Saving', episode)\n",
    "    \n",
    "    episode += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 854\n",
      "26.46370023419204\n"
     ]
    }
   ],
   "source": [
    "print(len(total_kills), len(total_deaths))\n",
    "print(len(total_kills)/len(total_deaths)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the model based the generated data\n",
    "\n",
    "\n",
    "def keras_model(inputs, dropouts=0.05, opti=1e-6, clipping=None, clipval=None):\n",
    "    keras_model = Sequential()\n",
    "    \n",
    "    keras_model.add(Dense(128, input_dim=inputs, \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=1.0, stddev=0.1), \n",
    "                          activation='relu'))\n",
    "    #keras_model.add(advanced_activations.LeakyReLU(alpha=0.01))\n",
    "    #keras_model.add(advanced_activations.ELU(alpha=1))\n",
    "    keras_model.add(Dropout(dropouts))\n",
    "    \n",
    "    keras_model.add(Dense(256, input_dim=inputs, \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=1.0, stddev=0.1),\n",
    "                          activation='relu'))\n",
    "    #keras_model.add(advanced_activations.LeakyReLU(alpha=0.01))\n",
    "    #keras_model.add(advanced_activations.ELU(alpha=1))\n",
    "    keras_model.add(Dropout(dropouts))\n",
    "    \n",
    "    keras_model.add(Dense(512, input_dim=inputs, \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=1.0, stddev=0.1),\n",
    "                          activation='relu'))\n",
    "    #keras_model.add(advanced_activations.LeakyReLU(alpha=0.01))\n",
    "    #keras_model.add(advanced_activations.ELU(alpha=1))\n",
    "    keras_model.add(Dropout(dropouts))\n",
    "    \n",
    "    keras_model.add(Dense(256, input_dim=inputs, \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=1.0, stddev=0.1),\n",
    "                          activation='relu'))\n",
    "    #keras_model.add(advanced_activations.LeakyReLU(alpha=0.01))\n",
    "    #keras_model.add(advanced_activations.ELU(alpha=1))\n",
    "    keras_model.add(Dropout(dropouts))\n",
    "    \n",
    "    keras_model.add(Dense(128, input_dim=inputs, \n",
    "                          kernel_initializer=initializers.RandomNormal(mean=1.0, stddev=0.1),\n",
    "                          activation='relu'))\n",
    "    #keras_model.add(advanced_activations.LeakyReLU(alpha=0.01))\n",
    "    #keras_model.add(advanced_activations.ELU(alpha=1))\n",
    "    keras_model.add(Dropout(dropouts))\n",
    "    \n",
    "    keras_model.add(Dense(1))\n",
    "    adam = Adam(lr=opti, clipnorm=clipping, clipvalue=clipval)\n",
    "    keras_model.compile(loss='mse', optimizer='adam')\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#connect to DB\n",
    "\n",
    "con = sqlite3.connect(\"SquareBrawlSQLite_AdjustedRewards.db\")\n",
    "\n",
    "SQL_STATEMENT = '''SELECT\n",
    "    p1_x,\n",
    "    \" p1_y\",\n",
    "    p2_x,\n",
    "    \" p2_y\",\n",
    "    right_of, \n",
    "    left_of, \n",
    "    above, \n",
    "    below, \n",
    "    x_clear, \n",
    "    y_clear, \n",
    "    cd1,\n",
    "    cd2, \n",
    "    aim_down, \n",
    "    aim_left, \n",
    "    aim_up, \n",
    "    aim_right,\n",
    "    action,\n",
    "    reward_kill_adjusted,\n",
    "    aim_rewards,\n",
    "    adjusted_rewards_total,\n",
    "    p2_score\n",
    "FROM\n",
    "    TD10_decay9_pos_neg_reward_1_looking_reward\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9735, 24)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove memory errors from dataset AFTER the rewards have been allocated\n",
    "def drop_memory_errors(df):\n",
    "    '''\n",
    "    Some features generate errors so the whole row would be blank.\n",
    "    This function removes those errrors from the training set.\n",
    "    '''\n",
    "    p1 = df['p1_x'] + df[' p1_y']\n",
    "    p2 = df['p2_x'] + df[' p2_y']\n",
    "    df['memory_error'] = p1*p2\n",
    "    df['memory_error'] = df['memory_error']\n",
    "    df = df[df['memory_error'] != 0]\n",
    "    df.drop('memory_error', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 17\n"
     ]
    }
   ],
   "source": [
    "chunksizes = 500\n",
    "drop_cols = ['reward_kill_adjusted' , 'aim_rewards', 'adjusted_rewards_total', 'p2_score']\n",
    "\n",
    "training_chunks_clean = pd.read_sql(SQL_STATEMENT, con, chunksize=chunksizes)\n",
    "\n",
    "# grab out input shape size\n",
    "for chunk in training_chunks_clean:\n",
    "    input_shape = chunk.shape[1] - len(drop_cols) # minus 1 for y target which is to be dropped\n",
    "    print('input shape', input_shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Qas = keras_model(inputs=input_shape, dropouts=0.1, opti=1e-6, clipping=None, clipval=None)\n",
    "#Qas.load_weights('model_weights//total_rewards_v2//1500000_model_backup_20170626_1614.keras')\n",
    "\n",
    "save_location = 'model_weights//testing//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               2304      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 331,265\n",
      "Trainable params: 331,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Qas.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksizes = 1000000\n",
    "training_chunks_clean = pd.read_sql(SQL_STATEMENT, con, chunksize=chunksizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\envs\\sb\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "21540/21540 [==============================] - 6s - loss: 111137485233854152704.0000     \n",
      "Epoch 2/150\n",
      "21540/21540 [==============================] - 6s - loss: 42656036448617758720.0000     \n",
      "Epoch 3/150\n",
      "21540/21540 [==============================] - 6s - loss: 18559712370211590144.0000     \n",
      "Epoch 4/150\n",
      "21540/21540 [==============================] - 6s - loss: 17785592289470771200.0000     \n",
      "Epoch 5/150\n",
      "21540/21540 [==============================] - 6s - loss: 7178877202275830784.0000     \n",
      "Epoch 6/150\n",
      "21540/21540 [==============================] - 6s - loss: 3825409194559627264.0000     \n",
      "Epoch 7/150\n",
      "21540/21540 [==============================] - 6s - loss: 2773497644701135360.0000     \n",
      "Epoch 8/150\n",
      "21540/21540 [==============================] - 6s - loss: 2096969625110181632.0000     \n",
      "Epoch 9/150\n",
      "21540/21540 [==============================] - 6s - loss: 1630730253757091584.0000     \n",
      "Epoch 10/150\n",
      "21540/21540 [==============================] - 6s - loss: 1327245439448315392.0000     \n",
      "Epoch 11/150\n",
      "21540/21540 [==============================] - 6s - loss: 399626140052509696.0000     \n",
      "Epoch 12/150\n",
      "21540/21540 [==============================] - 6s - loss: 437340752025362688.0000     \n",
      "Epoch 13/150\n",
      "21540/21540 [==============================] - 6s - loss: 648919275433643392.0000     \n",
      "Epoch 14/150\n",
      "21540/21540 [==============================] - 6s - loss: 242187249257548704.0000     \n",
      "Epoch 15/150\n",
      "21540/21540 [==============================] - 6s - loss: 266245285509630816.0000     \n",
      "Epoch 16/150\n",
      "21540/21540 [==============================] - 6s - loss: 146638165924307680.0000     \n",
      "Epoch 17/150\n",
      "21540/21540 [==============================] - 6s - loss: 72830107965865952.0000     \n",
      "Epoch 18/150\n",
      "21540/21540 [==============================] - 6s - loss: 85870187078039424.0000     \n",
      "Epoch 19/150\n",
      "21540/21540 [==============================] - 6s - loss: 43795817366889264.0000     \n",
      "Epoch 20/150\n",
      "21540/21540 [==============================] - 6s - loss: 16808562921257836.0000     \n",
      "Epoch 21/150\n",
      "21540/21540 [==============================] - 6s - loss: 33358908616459888.0000     \n",
      "Epoch 22/150\n",
      "21540/21540 [==============================] - 6s - loss: 17813904315120852.0000     \n",
      "Epoch 23/150\n",
      "21540/21540 [==============================] - 6s - loss: 12256137052316266.0000     \n",
      "Epoch 24/150\n",
      "21540/21540 [==============================] - 6s - loss: 5252218811100699.0000     \n",
      "Epoch 25/150\n",
      "21540/21540 [==============================] - 6s - loss: 3635776133410688.0000     \n",
      "Epoch 26/150\n",
      "21540/21540 [==============================] - 6s - loss: 1572115592692804.2500     \n",
      "Epoch 27/150\n",
      "21540/21540 [==============================] - 6s - loss: 1359506689538516.0000     \n",
      "Epoch 28/150\n",
      "21540/21540 [==============================] - 6s - loss: 1943736242066604.7500     \n",
      "Epoch 29/150\n",
      "21540/21540 [==============================] - 6s - loss: 1620250656879329.7500     \n",
      "Epoch 30/150\n",
      "21540/21540 [==============================] - 6s - loss: 858666517380701.3750     \n",
      "Epoch 31/150\n",
      "21540/21540 [==============================] - 6s - loss: 437899106641913.9375     \n",
      "Epoch 32/150\n",
      "21540/21540 [==============================] - 6s - loss: 249311322068046.0312     \n",
      "Epoch 33/150\n",
      "21540/21540 [==============================] - 6s - loss: 154406492271707.4375     \n",
      "Epoch 34/150\n",
      "21540/21540 [==============================] - 6s - loss: 198985933037132.3438     \n",
      "Epoch 35/150\n",
      "21540/21540 [==============================] - 6s - loss: 75099750901385.7812     \n",
      "Epoch 36/150\n",
      "21540/21540 [==============================] - 6s - loss: 96720641441713.9062     \n",
      "Epoch 37/150\n",
      "21540/21540 [==============================] - 6s - loss: 75724595823316.5625     \n",
      "Epoch 38/150\n",
      "21540/21540 [==============================] - 6s - loss: 6865498275014.9512     \n",
      "Epoch 39/150\n",
      "21540/21540 [==============================] - 6s - loss: 15631960206505.6367     \n",
      "Epoch 40/150\n",
      "21540/21540 [==============================] - 6s - loss: 12026483413500.0078     \n",
      "Epoch 41/150\n",
      "21540/21540 [==============================] - 6s - loss: 7133825962220.3252     \n",
      "Epoch 42/150\n",
      "21540/21540 [==============================] - 6s - loss: 3283940462575.8340     \n",
      "Epoch 43/150\n",
      "21540/21540 [==============================] - 6s - loss: 1774697967199.6384     \n",
      "Epoch 44/150\n",
      "21540/21540 [==============================] - 6s - loss: 2200490639753.0596     \n",
      "Epoch 45/150\n",
      "21540/21540 [==============================] - 6s - loss: 847577269402.3146     \n",
      "Epoch 46/150\n",
      "21540/21540 [==============================] - 6s - loss: 346974543881.9789     \n",
      "Epoch 47/150\n",
      "21540/21540 [==============================] - 6s - loss: 332617048615.1005     \n",
      "Epoch 48/150\n",
      "21540/21540 [==============================] - 6s - loss: 160704296454.1838     \n",
      "Epoch 49/150\n",
      "21540/21540 [==============================] - 6s - loss: 114233398186.2583     \n",
      "Epoch 50/150\n",
      "21540/21540 [==============================] - 6s - loss: 46430763448.2069     \n",
      "Epoch 51/150\n",
      "21540/21540 [==============================] - 6s - loss: 33275053952.1803     \n",
      "Epoch 52/150\n",
      "21540/21540 [==============================] - 6s - loss: 18285924625.0614     \n",
      "Epoch 53/150\n",
      "21540/21540 [==============================] - 6s - loss: 3652079439.9659     \n",
      "Epoch 54/150\n",
      "21540/21540 [==============================] - 6s - loss: 2477077144.4805     \n",
      "Epoch 55/150\n",
      "21540/21540 [==============================] - 7s - loss: 2067694717.2557     \n",
      "Epoch 56/150\n",
      "21540/21540 [==============================] - 7s - loss: 740595619.6042     \n",
      "Epoch 57/150\n",
      "21540/21540 [==============================] - 7s - loss: 353411940.9287     \n",
      "Epoch 58/150\n",
      "21540/21540 [==============================] - 7s - loss: 302741608.2373     \n",
      "Epoch 59/150\n",
      "21540/21540 [==============================] - 7s - loss: 89347679.7643     \n",
      "Epoch 60/150\n",
      "21540/21540 [==============================] - 7s - loss: 209195079.9153     \n",
      "Epoch 61/150\n",
      "21540/21540 [==============================] - 8s - loss: 19063070.9675     \n",
      "Epoch 62/150\n",
      "21540/21540 [==============================] - 10s - loss: 20611374.8231    \n",
      "Epoch 63/150\n",
      "21540/21540 [==============================] - 11s - loss: 63441079.4923    \n",
      "Epoch 64/150\n",
      "21540/21540 [==============================] - 11s - loss: 8910869.1657    \n",
      "Epoch 65/150\n",
      "21540/21540 [==============================] - 16s - loss: 6746049.9145    \n",
      "Epoch 66/150\n",
      "21540/21540 [==============================] - 21s - loss: 1599832.6216    \n",
      "Epoch 67/150\n",
      "21540/21540 [==============================] - 18s - loss: 3814180.7701    \n",
      "Epoch 68/150\n",
      "21540/21540 [==============================] - 19s - loss: 438121.5828    \n",
      "Epoch 69/150\n",
      "21540/21540 [==============================] - 25s - loss: 6574046.0215    \n",
      "Epoch 70/150\n",
      "21540/21540 [==============================] - 24s - loss: 1018312.1743    \n",
      "Epoch 71/150\n",
      "21540/21540 [==============================] - 30s - loss: 675116.3155    \n",
      "Epoch 72/150\n",
      "21540/21540 [==============================] - 30s - loss: 1422918.1008    \n",
      "Epoch 73/150\n",
      "21540/21540 [==============================] - 37s - loss: 1873815.6302    \n",
      "Epoch 74/150\n",
      "21540/21540 [==============================] - 47s - loss: 206.2929    \n",
      "Epoch 75/150\n",
      "21540/21540 [==============================] - 55s - loss: 0.1969    \n",
      "Epoch 76/150\n",
      "21540/21540 [==============================] - 53s - loss: 17222.9302    \n",
      "Epoch 77/150\n",
      "21540/21540 [==============================] - 41s - loss: 0.1147    \n",
      "Epoch 78/150\n",
      "21540/21540 [==============================] - 62s - loss: 483.6640    \n",
      "Epoch 79/150\n",
      "21540/21540 [==============================] - 54s - loss: 347580.1109    \n",
      "Epoch 80/150\n",
      "21540/21540 [==============================] - 48s - loss: 210.1184    \n",
      "Epoch 81/150\n",
      "21540/21540 [==============================] - 61s - loss: 1.8042    \n",
      "Epoch 82/150\n",
      "21540/21540 [==============================] - 62s - loss: 0.1206    \n",
      "Epoch 83/150\n",
      "21540/21540 [==============================] - 62s - loss: 101.4809    \n",
      "Epoch 84/150\n",
      "21540/21540 [==============================] - 60s - loss: 4.4382    \n",
      "Epoch 85/150\n",
      "21540/21540 [==============================] - 61s - loss: 120.2369    \n",
      "Epoch 86/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0740    \n",
      "Epoch 87/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.1574    \n",
      "Epoch 88/150\n",
      "21540/21540 [==============================] - 65s - loss: 5.1187    \n",
      "Epoch 89/150\n",
      "21540/21540 [==============================] - 63s - loss: 0.0432    \n",
      "Epoch 90/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0860    \n",
      "Epoch 91/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.0939    \n",
      "Epoch 92/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.0147    \n",
      "Epoch 93/150\n",
      "21540/21540 [==============================] - 64s - loss: 22.3806    \n",
      "Epoch 94/150\n",
      "21540/21540 [==============================] - 64s - loss: 0.0247    - ETA: 1s\n",
      "Epoch 95/150\n",
      "21540/21540 [==============================] - 61s - loss: 0.7389    \n",
      "Epoch 96/150\n",
      "21540/21540 [==============================] - 60s - loss: 6.9712    \n",
      "Epoch 97/150\n",
      "21540/21540 [==============================] - 61s - loss: 0.0130    \n",
      "Epoch 98/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.2917    \n",
      "Epoch 99/150\n",
      "21540/21540 [==============================] - 60s - loss: 40.0165    \n",
      "Epoch 100/150\n",
      "21540/21540 [==============================] - 63s - loss: 0.0129    \n",
      "Epoch 101/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 102/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    - ETA: 1s - \n",
      "Epoch 103/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 104/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0130    - ETA: 1s -\n",
      "Epoch 105/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 106/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 107/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 108/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 109/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 110/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 111/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 112/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 113/150\n",
      "21540/21540 [==============================] - 63s - loss: 6.9408    \n",
      "Epoch 114/150\n",
      "21540/21540 [==============================] - 60s - loss: 1.2238    \n",
      "Epoch 115/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0128    - ETA: 1s\n",
      "Epoch 116/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.0210    \n",
      "Epoch 117/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 118/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 119/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0129    \n",
      "Epoch 120/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 121/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 122/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 123/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    - E - ETA: 1s - l\n",
      "Epoch 124/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 125/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 126/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 127/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 128/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 129/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.2159    \n",
      "Epoch 130/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0128    \n",
      "Epoch 131/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0171    \n",
      "Epoch 132/150\n",
      "21540/21540 [==============================] - 66s - loss: 0.0133    \n",
      "Epoch 133/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.2894    \n",
      "Epoch 134/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0128    \n",
      "Epoch 135/150\n",
      "21540/21540 [==============================] - 64s - loss: 70.3688    \n",
      "Epoch 136/150\n",
      "21540/21540 [==============================] - 62s - loss: 0.0137    \n",
      "Epoch 137/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0130    \n",
      "Epoch 138/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0128    \n",
      "Epoch 139/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 140/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 141/150\n",
      "21540/21540 [==============================] - 66s - loss: 197.2213    \n",
      "Epoch 142/150\n",
      "21540/21540 [==============================] - 61s - loss: 757.7646    \n",
      "Epoch 143/150\n",
      "21540/21540 [==============================] - 61s - loss: 0.0129    \n",
      "Epoch 144/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0129    \n",
      "Epoch 145/150\n",
      "21540/21540 [==============================] - 67s - loss: 0.0150    \n",
      "Epoch 146/150\n",
      "21540/21540 [==============================] - 64s - loss: 1.0883    \n",
      "Epoch 147/150\n",
      "21540/21540 [==============================] - 65s - loss: 0.0128    \n",
      "Epoch 148/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0128    \n",
      "Epoch 149/150\n",
      "21540/21540 [==============================] - 68s - loss: 0.0130    \n",
      "Epoch 150/150\n",
      "21540/21540 [==============================] - 66s - loss: 12.8862    \n",
      "Saving  model_weights//testing//1000000_model_backup_20170630_0144.keras\n",
      "----------------------|  66.55574043261231 % done |---------------------------------\n",
      "Epoch 1/150\n",
      "19654/19654 [==============================] - 48s - loss: 6966.7886    \n",
      "Epoch 2/150\n",
      "19654/19654 [==============================] - 51s - loss: 0.0156    \n",
      "Epoch 3/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0152    \n",
      "Epoch 4/150\n",
      "19654/19654 [==============================] - 60s - loss: 0.2175    \n",
      "Epoch 5/150\n",
      "19654/19654 [==============================] - 60s - loss: 0.0159    \n",
      "Epoch 6/150\n",
      "19654/19654 [==============================] - 60s - loss: 0.0139    \n",
      "Epoch 7/150\n",
      "19654/19654 [==============================] - 61s - loss: 0.0137    \n",
      "Epoch 8/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0136    - ETA: \n",
      "Epoch 9/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 10/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 11/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 12/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 13/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 14/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 15/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 16/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 17/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 18/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 1s - l\n",
      "Epoch 19/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 0s - lo\n",
      "Epoch 20/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 21/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 22/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 23/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 24/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 25/150\n",
      "19654/19654 [==============================] - 59s - loss: 4.6744    \n",
      "Epoch 26/150\n",
      "19654/19654 [==============================] - 60s - loss: 0.0135    \n",
      "Epoch 27/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 1s - l\n",
      "Epoch 28/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 29/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 30/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 31/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135     - ETA: 1\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19654/19654 [==============================] - 61s - loss: 0.0135    - ETA: 0s - loss:\n",
      "Epoch 33/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 34/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 35/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA:\n",
      "Epoch 36/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 37/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 38/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 39/150\n",
      "19654/19654 [==============================] - 61s - loss: 0.0135    - ETA: 1s \n",
      "Epoch 40/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 41/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 42/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 43/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 44/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 45/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 1s - l\n",
      "Epoch 46/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 47/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 48/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 49/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 50/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 51/150\n",
      "19654/19654 [==============================] - ETA: 0s - loss: 0.013 - 62s - loss: 0.0135    \n",
      "Epoch 52/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 53/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ET\n",
      "Epoch 54/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 55/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 56/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 57/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 58/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 0s - loss\n",
      "Epoch 59/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 60/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 61/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 62/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 63/150\n",
      "19654/19654 [==============================] - 61s - loss: 6.1228    \n",
      "Epoch 64/150\n",
      "19654/19654 [==============================] - 58s - loss: 0.0135    \n",
      "Epoch 65/150\n",
      "19654/19654 [==============================] - ETA: 0s - loss: 0.0135- ETA: 3s - l - 61s - loss: 0.0135    \n",
      "Epoch 66/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 67/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 68/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 0s - loss: 0\n",
      "Epoch 69/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 70/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 71/150\n",
      "19654/19654 [==============================] - 57s - loss: 1220.2653    \n",
      "Epoch 72/150\n",
      "19654/19654 [==============================] - 59s - loss: 0.0141    \n",
      "Epoch 73/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0138    - ETA: 1s \n",
      "Epoch 74/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0137    \n",
      "Epoch 75/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0136    \n",
      "Epoch 76/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 77/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 78/150\n",
      "19654/19654 [==============================] - 64s - loss: 0.0135    \n",
      "Epoch 79/150\n",
      "19654/19654 [==============================] - 65s - loss: 0.0135    \n",
      "Epoch 80/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 81/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 82/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 83/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 84/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 85/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 86/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 87/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 1\n",
      "Epoch 88/150\n",
      "19654/19654 [==============================] - 60s - loss: 0.9448    \n",
      "Epoch 89/150\n",
      "19654/19654 [==============================] - 61s - loss: 0.0135    \n",
      "Epoch 90/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 91/150\n",
      "19654/19654 [==============================] - 60s - loss: 52.3890    \n",
      "Epoch 92/150\n",
      "19654/19654 [==============================] - 59s - loss: 0.0137    \n",
      "Epoch 93/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 94/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 95/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 96/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: 1s -\n",
      "Epoch 97/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 98/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 99/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 100/150\n",
      "19654/19654 [==============================] - 61s - loss: 0.0137    \n",
      "Epoch 101/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    - ETA: \n",
      "Epoch 102/150\n",
      "19654/19654 [==============================] - 62s - loss: 0.0135    \n",
      "Epoch 103/150\n",
      "19654/19654 [==============================] - 64s - loss: 0.0135    \n",
      "Epoch 104/150\n",
      "19654/19654 [==============================] - 77s - loss: 0.0135    \n",
      "Epoch 105/150\n",
      "19654/19654 [==============================] - 88s - loss: 0.0135    \n",
      "Epoch 106/150\n",
      "19654/19654 [==============================] - 89s - loss: 0.0135    \n",
      "Epoch 107/150\n",
      "19654/19654 [==============================] - 89s - loss: 0.0135    \n",
      "Epoch 108/150\n",
      "19654/19654 [==============================] - 89s - loss: 0.0135    \n",
      "Epoch 109/150\n",
      "19654/19654 [==============================] - 90s - loss: 0.0135    \n",
      "Epoch 110/150\n",
      "19654/19654 [==============================] - 90s - loss: 0.0135    \n",
      "Epoch 111/150\n",
      "19654/19654 [==============================] - 91s - loss: 0.0135    \n",
      "Epoch 112/150\n",
      "19654/19654 [==============================] - 91s - loss: 0.0135    \n",
      "Epoch 113/150\n",
      "19654/19654 [==============================] - 91s - loss: 0.0135    \n",
      "Epoch 114/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 115/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 116/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 117/150\n",
      "19654/19654 [==============================] - 92s - loss: 2.0601    \n",
      "Epoch 118/150\n",
      "19654/19654 [==============================] - 90s - loss: 0.0135    \n",
      "Epoch 119/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 120/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 121/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 122/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 123/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 124/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 125/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 126/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0269    \n",
      "Epoch 127/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 128/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 129/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 130/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 131/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 132/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 133/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 134/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 135/150\n",
      "19654/19654 [==============================] - 94s - loss: 0.0135    \n",
      "Epoch 136/150\n",
      "19654/19654 [==============================] - 98s - loss: 0.0135    \n",
      "Epoch 137/150\n",
      "19654/19654 [==============================] - 95s - loss: 0.0135    \n",
      "Epoch 138/150\n",
      "19654/19654 [==============================] - 91s - loss: 1.7823    \n",
      "Epoch 139/150\n",
      "19654/19654 [==============================] - 91s - loss: 0.0135    \n",
      "Epoch 140/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 141/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 142/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 143/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 144/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.0135    \n",
      "Epoch 145/150\n",
      "19654/19654 [==============================] - 93s - loss: 0.3627    \n",
      "Epoch 146/150\n",
      "19654/19654 [==============================] - 92s - loss: 0.0135    \n",
      "Epoch 147/150\n",
      "19654/19654 [==============================] - 94s - loss: 0.0135    \n",
      "Epoch 148/150\n",
      "19654/19654 [==============================] - 94s - loss: 0.0135    \n",
      "Epoch 149/150\n",
      "19654/19654 [==============================] - 94s - loss: 0.0135    \n",
      "Epoch 150/150\n",
      "19654/19654 [==============================] - 94s - loss: 0.0135    \n",
      "Saving  model_weights//testing//2000000_model_backup_20170630_0443.keras\n",
      "----------------------|  99.83361064891847 % done |---------------------------------\n",
      "Epoch 1/150\n",
      "19415/19415 [==============================] - 94s - loss: 0.0139    \n",
      "Epoch 2/150\n",
      "19415/19415 [==============================] - 94s - loss: 0.0139    \n",
      "Epoch 3/150\n",
      "19415/19415 [==============================] - 94s - loss: 0.0139    \n",
      "Epoch 4/150\n",
      "19415/19415 [==============================] - 95s - loss: 0.0139    \n",
      "Epoch 5/150\n",
      "19415/19415 [==============================] - 95s - loss: 0.0139    \n",
      "Epoch 6/150\n",
      "19415/19415 [==============================] - 96s - loss: 0.0139    \n",
      "Epoch 7/150\n",
      "19415/19415 [==============================] - 96s - loss: 0.0139    \n",
      "Epoch 8/150\n",
      "19415/19415 [==============================] - 97s - loss: 0.0139    \n",
      "Epoch 9/150\n",
      "19415/19415 [==============================] - 98s - loss: 0.0139    \n",
      "Epoch 10/150\n",
      "19415/19415 [==============================] - 98s - loss: 0.0139    \n",
      "Epoch 11/150\n",
      "19415/19415 [==============================] - 98s - loss: 0.0139    \n",
      "Epoch 12/150\n",
      "19415/19415 [==============================] - 99s - loss: 0.0139    \n",
      "Epoch 13/150\n",
      "19415/19415 [==============================] - 99s - loss: 0.0139    \n",
      "Epoch 14/150\n",
      "19415/19415 [==============================] - 99s - loss: 0.0139    \n",
      "Epoch 15/150\n",
      "19415/19415 [==============================] - 99s - loss: 0.0139    \n",
      "Epoch 16/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 17/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 18/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 19/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 20/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 21/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 22/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 23/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 24/150\n",
      "19415/19415 [==============================] - 99s - loss: 74.8712    \n",
      "Epoch 25/150\n",
      "19415/19415 [==============================] - 95s - loss: 0.0144    \n",
      "Epoch 26/150\n",
      "19415/19415 [==============================] - 99s - loss: 0.0140    \n",
      "Epoch 27/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 28/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 29/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 30/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 31/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.4040   \n",
      "Epoch 32/150\n",
      "19415/19415 [==============================] - 98s - loss: 0.0139    \n",
      "Epoch 33/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 34/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 35/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 36/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 37/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 38/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 39/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 40/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 41/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 42/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 43/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 44/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 45/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 46/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 47/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 48/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 49/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 50/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 51/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 52/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 53/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 54/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 55/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 56/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 57/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 58/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 59/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 60/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 61/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 62/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 63/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 64/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 65/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 66/150\n",
      "19415/19415 [==============================] - 100s - loss: 0.0139   \n",
      "Epoch 67/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 68/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 69/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 70/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 71/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0143   \n",
      "Epoch 72/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 73/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 75/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 76/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 77/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 78/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 79/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 80/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 81/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 82/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 83/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 84/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 85/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 86/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 87/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 88/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 89/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 90/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 91/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 92/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 93/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 94/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 95/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 96/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 97/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 98/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 99/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 100/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 101/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 102/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 103/150\n",
      "19415/19415 [==============================] - 101s - loss: 0.0139   \n",
      "Epoch 104/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 105/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 106/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 107/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 108/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 109/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 110/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 111/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 112/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 113/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 114/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 115/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 116/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 117/150\n",
      "19415/19415 [==============================] - 105s - loss: 0.0139   \n",
      "Epoch 118/150\n",
      "19415/19415 [==============================] - 109s - loss: 0.0139   \n",
      "Epoch 119/150\n",
      "19415/19415 [==============================] - 110s - loss: 0.0139   \n",
      "Epoch 120/150\n",
      "19415/19415 [==============================] - 110s - loss: 0.0139   \n",
      "Epoch 121/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 122/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 123/150\n",
      "19415/19415 [==============================] - 114s - loss: 0.0139   \n",
      "Epoch 124/150\n",
      "19415/19415 [==============================] - 114s - loss: 0.0139   \n",
      "Epoch 125/150\n",
      "19415/19415 [==============================] - 113s - loss: 0.0139   \n",
      "Epoch 126/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 127/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 128/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 129/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 130/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 131/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 132/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 133/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 134/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 135/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 136/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 137/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 138/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 139/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 140/150\n",
      "19415/19415 [==============================] - 112s - loss: 0.0139   \n",
      "Epoch 141/150\n",
      "19415/19415 [==============================] - 111s - loss: 0.0139   \n",
      "Epoch 142/150\n",
      "19415/19415 [==============================] - 110s - loss: 0.0139   \n",
      "Epoch 143/150\n",
      "19415/19415 [==============================] - 105s - loss: 0.0139   \n",
      "Epoch 144/150\n",
      "19415/19415 [==============================] - 103s - loss: 0.0139   \n",
      "Epoch 145/150\n",
      "19415/19415 [==============================] - 102s - loss: 0.0139   \n",
      "Epoch 146/150\n",
      "19415/19415 [==============================] - 105s - loss: 0.0139   \n",
      "Epoch 147/150\n",
      "19415/19415 [==============================] - 107s - loss: 0.0139   \n",
      "Epoch 148/150\n",
      "19415/19415 [==============================] - 115s - loss: 0.0139   \n",
      "Epoch 149/150\n",
      "19415/19415 [==============================] - 106s - loss: 0.0139   \n",
      "Epoch 150/150\n",
      "19415/19415 [==============================] - 103s - loss: 0.0139   \n",
      "Saving  model_weights//testing//3000000_model_backup_20170630_0901.keras\n",
      "----------------------|  133.11148086522462 % done |---------------------------------\n",
      "Epoch 1/150\n",
      "5721/5721 [==============================] - 31s - loss: 0.0124    \n",
      "Epoch 2/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 3/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 4/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 5/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 6/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 7/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 8/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 9/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 10/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0123    \n",
      "Epoch 11/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 12/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 13/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0123    \n",
      "Epoch 14/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 15/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 16/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 17/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 18/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 19/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 20/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 21/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 22/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 23/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 24/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 25/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 26/150\n",
      "5721/5721 [==============================] - 32s - loss: 0.0124    \n",
      "Epoch 27/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 28/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 29/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 30/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 31/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 32/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 33/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 34/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 35/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 36/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 37/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 38/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 39/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 40/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 41/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 42/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 43/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 44/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 45/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 46/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 47/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 48/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 49/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 50/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 51/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 52/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 53/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 54/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 55/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 56/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 57/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 58/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 59/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 60/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 61/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 62/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 63/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 64/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 65/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 66/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0123    \n",
      "Epoch 67/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 68/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 69/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 70/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 71/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 72/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 73/150\n",
      "5721/5721 [==============================] - 33s - loss: 0.0124    \n",
      "Epoch 74/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 75/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 76/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 77/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 78/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 79/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 80/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 81/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 82/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 83/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 84/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 85/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 86/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 87/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 88/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 89/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 90/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 91/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 92/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 93/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 94/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 95/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 96/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 97/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 98/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 99/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 100/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0123    \n",
      "Epoch 101/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 102/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 103/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 104/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 105/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 106/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 107/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 108/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 109/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 110/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 111/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 112/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 113/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 114/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0123    \n",
      "Epoch 115/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 116/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 117/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 118/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 119/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 120/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 122/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0123    \n",
      "Epoch 123/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 124/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 125/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 126/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 127/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 128/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 129/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 130/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0183    \n",
      "Epoch 131/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 132/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 133/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 134/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 135/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 136/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 137/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 138/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 139/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 140/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 141/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 142/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 143/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 144/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 145/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 146/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 147/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 148/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 149/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Epoch 150/150\n",
      "5721/5721 [==============================] - 34s - loss: 0.0124    \n",
      "Saving  model_weights//testing//4000000_model_backup_20170630_1025.keras\n",
      "----------------------|  166.38935108153078 % done |---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train keras model and save weights periodically\n",
    "episode = 1\n",
    "total_rows = 0\n",
    "adjusted_rows = 0\n",
    "\n",
    "#grab out input shape size\n",
    "for chunk in training_chunks_clean:\n",
    "    \n",
    "    #drop memory errors\n",
    "    total_rows += chunk.shape[0]\n",
    "    chunk = drop_memory_errors(chunk)\n",
    "    chunk = chunk[chunk['p2_score']<50]\n",
    "    adjusted_rows += chunk.shape[0]\n",
    "    \n",
    "    #randomize sample\n",
    "    #chunk = chunk.sample(frac=1, random_state=1337) # randomly mix everything\n",
    "    \n",
    "    y_val = chunk['reward_kill_adjusted'] #+ chunk['aim_rewards']\n",
    "    y = np.array(y_val)\n",
    "    \n",
    "    for drop in drop_cols:\n",
    "        chunk.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "    training_set = np.array(chunk)\n",
    "    Qas.fit(training_set, y, epochs=150, verbose=1)\n",
    "    \n",
    "    name =  save_location + str(episode*chunksizes) + '_model_backup_' + datetime.datetime.today().strftime('%Y%m%d_%H%M') + '.keras'\n",
    "    Qas.save_weights(name)\n",
    "    print('Saving ', name)\n",
    "    \n",
    "    episode += 1\n",
    "    \n",
    "    print('----------------------| ', episode*chunksizes/3005000*100, '% done |---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name =  save_location + str(episode*chunksizes) + '_model_backup_' + datetime.datetime.today().strftime('%Y%m%d_%H%M') + '.keras'\n",
    "Qas.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342532 3085000\n",
      "0.4351805510534846\n"
     ]
    }
   ],
   "source": [
    "print(adjusted_rows, total_rows)\n",
    "print(adjusted_rows / total_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
